











import seaborn as sns

tips = sns.load_dataset('tips')
tips                       





import matplotlib.pyplot as plt
import numpy as np

num = tips.shape[0]
plt.scatter(x=np.arange(num), y=tips["tip"], label="tip")
plt.axhline(y=np.mean(tips['tip']), color='r', linestyle='--', label='mean') # axhline can draw a horizonal line parallel to the axis
plt.legend()
plt.show()





np.mean(tips["tip"])





tips["tip"].mean()





np.var(tips["tip"])


tips["tip"].var()




















import numpy as np
import seaborn as sns

sns.set_theme()
data = np.random.uniform(100, 200, 10000) # generate 10000 random datas uniformally distributed between 50 and 100
sns.histplot(data=data) # use the histplot of seaborn to draw the histogram graph

















import numpy as np
import seaborn as sns

data = np.random.normal(100, 30, 10000) # generate 10000 random data normall distributed between 50 and 100
sns.histplot(data=data) # use the histplot of seaborn to draw the histogram graph























sns.regplot(data=tips, x="total_bill", y="tip")


tips['total_bill'].corr(tips['tip']) # compute the Pearson correlation











import matplotlib.pyplot as plt

sns.barplot(
    data=tips,
    x="size",
    y="tip", 
)


tips['size'].corr(tips['tip'], method="spearman") # compute the spearman correlation


import matplotlib.pyplot as plt

sns.barplot(
    data=tips,
    x="day",
    y="tip", 
)





import pandas as pd

tips["day_numeric"] = pd.factorize(tips["day"])[0] # [1] is the orginal catogorical data
tips


tips['day_numeric'].corr(tips['tip'], method="spearman") # compute the spearman correlation






























































from scipy import stats
import seaborn as sns

tips = sns.load_dataset('tips')
stats.ttest_1samp(tips['tip'], popmean=3)











tips = sns.load_dataset('tips')
stats.ttest_1samp(tips['tip'], popmean=3, alternative='less')




















import seaborn as sns
from scipy import stats

tips = sns.load_dataset("tips")
male_tip = tips[tips["sex"] == "Male"]["tip"]
female_tip = tips[tips["sex"] == "Female"]["tip"]

stats.ttest_ind(male_tip, female_tip)



































import pingouin as pg
import seaborn as sns

tips = sns.load_dataset("tips")
pg.anova(tips, dv="tip", between="sex")








pg.anova(tips, dv="tip", between="day")








pg.anova(tips, dv="tip", between=["sex", "smoker"])




















import pingouin as pg
import seaborn as sns

titanic = sns.load_dataset("titanic")
titanic


pg.chi2_independence(titanic, x="survived", y="sex")




















from sklearn.preprocessing import StandardScaler
import seaborn as sns

crashes = sns.load_dataset('car_crashes')
scaler = StandardScaler()
crash_standard = scaler.fit_transform(crashes.iloc[:, 0:-1]) # standarize the data except the last column
print(crash_standard[1:10]) # print the first 10 rows








from sklearn.preprocessing import MinMaxScaler
import seaborn as sns

crashes = sns.load_dataset('car_crashes')
scaler = MinMaxScaler()
crash_standard = scaler.fit_transform(crashes.iloc[:, 0:-1]) # standarize the data except the last column
print(crash_standard[1:10]) # print the first 10 rows










































































import numpy as np
import pandas as pd

# data collection
np.random.seed(123)
n = 300  # Sample size

# True scores of latent variables (used to generate observed items)
latent_ad = np.random.normal(5, 1, n)  # Latent variable: Ad Attractiveness
latent_trust = 0.6 * latent_ad + np.random.normal(
    0, 1, n
)  # Brand Trust (influenced by ads)
latent_purchase = (
    0.7 * latent_trust + 0.3 * latent_ad + np.random.normal(0, 1, n)
)  # Purchase Intention

# Generate observed items (with measurement error)
data = pd.DataFrame(
    {
        # Ad Attractiveness items (3 items)
        "ad1": latent_ad + np.random.normal(0, 0.5, n),
        "ad2": latent_ad + np.random.normal(0, 0.5, n),
        "ad3": latent_ad + np.random.normal(0, 0.5, n),
        # Brand Trust items (3 items)
        "trust1": latent_trust + np.random.normal(0, 0.5, n),
        "trust2": latent_trust + np.random.normal(0, 0.5, n),
        "trust3": latent_trust + np.random.normal(0, 0.5, n),
        # Purchase Intention items (2 items)
        "purchase1": latent_purchase + np.random.normal(0, 0.5, n),
        "purchase2": latent_purchase + np.random.normal(0, 0.5, n),
        # Moderator variable (Personal Innovativeness, 0 = Low, 1 = High)
        "group": np.random.binomial(1, 0.5, n),  # Random group assignment
    }
)


data























import pingouin as pg

# cronbach alpha test
cronbach_result = pg.cronbach_alpha(data=data)
print(f"cronbach alpha value is {cronbach_result[0]:.2f}")























import numpy as np
import pandas as pd
from semopy import Model, calc_stats

# Set random seed for reproducibility
np.random.seed(42)
n_samples = 300

# Generate latent variables with assumed relationships
# BrandImage ~ N(0,1)
BrandImage = np.random.normal(0, 1, n_samples)
# Customer Satisfaction influenced by BrandImage + noise
CustSatis = 0.7 * BrandImage + np.random.normal(0, 0.5, n_samples)
# Customer Loyalty influenced by BrandImage and CustSatis + noise
CustLoyal = 0.3 * BrandImage + 0.6 * CustSatis + np.random.normal(0, 0.5, n_samples)

# Function to generate observed indicators from latent variables
def generate_indicators(latent_var):
    # Each latent variable is measured by 3 indicators with measurement error (std=0.3)
    return latent_var.reshape(-1, 1) + np.random.normal(0, 0.3, (n_samples, 3))

# Generate observed variables for each latent variable
BI = generate_indicators(BrandImage)  # Brand Image indicators
CS = generate_indicators(CustSatis)   # Customer Satisfaction indicators
CL = generate_indicators(CustLoyal)   # Customer Loyalty indicators

# Combine all observed indicators into one dataset
data = np.hstack([BI, CS, CL])

# Create DataFrame with appropriate column names matching SEM model
columns = ['BI1', 'BI2', 'BI3', 'CS1', 'CS2', 'CS3', 'CL1', 'CL2', 'CL3']
df = pd.DataFrame(data, columns=columns)

# Transform data to simulate Likert-scale scores (1 to 5)
df = 3 + df          # Shift mean to around 3
df = df.clip(1, 5)   # Clip values to range [1, 5]
df = df.round(2)     # Round to 2 decimal places

print("Sample of simulated data:")
print(df.head())

# Define SEM model using lavaan-style syntax
model_desc = '''
BrandImage =~ BI1 + BI2 + BI3
CustSatis =~ CS1 + CS2 + CS3
CustLoyal =~ CL1 + CL2 + CL3

CustSatis ~ BrandImage
CustLoyal ~ BrandImage + CustSatis
'''

# Initialize and fit SEM model
model = Model(model_desc)
model.fit(df)

# Inspect and print path coefficients and loadings
print("\nPath coefficients and loadings:")
print(model.inspect())

# Calculate and print model fit statistics (CFI, TLI, RMSEA, AIC, BIC)
stats = calc_stats(model)
print("\nModel fit indices:")
print(stats[['CFI', 'TLI', 'RMSEA', 'AIC', 'BIC']])






















