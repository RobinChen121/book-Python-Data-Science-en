{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2ef5959",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 爬虫入门\n",
    "\n",
    "Python 的爬虫功能使得程序员可以快速抓取并分析网页中的信息，它实质上是模拟浏览器访问网页。本章主要常用的两个爬虫相关的库`requests`，`beautifulsoup4`。若要模拟鼠标点击等，要用到`selenium` 库，限于篇幅限制，本章不再介绍，具体可以查阅相关资料。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbbb767",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## `requests` 库\n",
    "\n",
    "每个网页都有源代码，可以通过鼠标单击右键查看网页源代码。网页中的很多信息都在源代码里面，`requests` 是一个访问网页源代码的库。一般通过 get 函数访问网页，另外一个常用来访问网页的函数是 post，与 get 函数的区别在于 post 能够传递表格或文件到网页所在服务器上。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d386cb6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table>   \n",
    "    <tr style=\"border-top:solid; border-bottom:solid\">\n",
    "            <th colspan=2 style=\"text-align:center\">get(url, [timeout], [headers], [proxies], **kwargs)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">url</td>\n",
    "        <td style=\"text-align:left\">网页链接</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">timeout</td>\n",
    "        <td style=\"text-align:left\">可选参数，请求网页的最大时长，单位为秒</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">headers</td>\n",
    "        <td style=\"text-align:left\">可选参数，模拟浏览器设置</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">proxies</td>\n",
    "        <td style=\"text-align:left\">可选参数，代理服务器设置</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom:solid\">\n",
    "        <td style=\"text-align:left\">**kwargs</td>\n",
    "        <td style=\"text-align:left\">其他参数</td>\n",
    "    </tr>\n",
    "</table>\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b589f11f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "get 或 post 函数返回一个 Resoponse 对象，该对象包括以下常用的属性或函数。\n",
    "\n",
    "|属性或函数|描述|\n",
    "|:--|:--|\n",
    "status_code|网页请求的返回状态，200 表示连接成功，404 表示连接失败|\n",
    "text|响应网页的字符串内容\n",
    "encoding|响应网页的编码方式，可以更改\n",
    "content|相应网页的字节形式内容，例如图片或 pdf 文件等\n",
    "raise_for_status()| 如果网页访问不成功，抛出异常，一般结合 try-except 语句使用\n",
    "json() | 该函数可以解析网页内容中 JSON 格式的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0edd6555",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get('http://www.baidu.com')\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873e6bef",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```{code-block}python\n",
    "r.text # 结果省略\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fddc805f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ISO-8859-1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.encoding  # 若为 ISO-8859-1 则中文为乱码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "099932fe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "r.encoding = 'utf-8' # 无论网页原内容是什么编码，都改成 utf-8 编码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e46dba7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## `beautifulsoup4`库\n",
    "\n",
    "使用`requests`获取的网页源代码一般非常复杂，不仅包括常规内容，还包括很多定义页面格式的代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f4c6db",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`````{admonition} tips\n",
    ":class: tip\n",
    "- 网页中的内容一般在网页源代码的各个标签里\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4c09d4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "例如有下面的 html 代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faee552e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "html_doc = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "        <h1>Hello, BeautifulSoup!</h1>\n",
    "        <ul>\n",
    "            <li><a class = \"c1\" href=\"http://example1.com\">Link 1</a></li>\n",
    "            <li><a class = \"c2\" href=\"http://example2.org\">Link 2</a></li>\n",
    "        </ul>\n",
    "        <div>\n",
    "        <p>text1</p> <p>text2</p>\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac7df72",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`beautifulsoup4`库提供了大量的属性或函数，能够方便地将网页（html）不同标签（tag）中的内容提取出来。常用的属性有下面几个："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae03272",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "|属性|描述|\n",
    "|:--|:--|\n",
    "head|网页源代码中的 &lt;head&gt; 标签内容\n",
    "title|网页源代码中的 &lt;title&gt; 标签内容\n",
    "body|网页源代码中的 &lt;body&gt; 标签内容\n",
    "p|网页源代码中的第一个 &lt;p&gt; 标签内容\n",
    "a|网页源代码中的第一个 &lt;a&gt; 标签内容\n",
    "div| 网页源代码中的第一个 &lt;div&gt; 标签内容\n",
    "script| 网页源代码中的第一个 &lt;script&gt; 标签内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "680f928c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "r = requests.get('http://www.baidu.com')\n",
    "r.encoding = 'utf-8'\n",
    "soup = BeautifulSoup(r.text) # 将网页内容传递给 BeautifulSoup 提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c0995f8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>百度一下，你就知道</title>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d2a5dff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p id=\"lh\"> <a href=\"http://home.baidu.com\">关于百度</a> <a href=\"http://ir.baidu.com\">About Baidu</a> </p>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.p # html 语言中 p 标签表示一个文本段落"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1dc8fb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "上面的这些标签对象还有自己的属性可以访问更具体的内容。\n",
    "\n",
    "|标签属性|描述|\n",
    "|:--|:--|\n",
    "name|标签名字，例如 a，p，div等\n",
    "attrs|标签的具体属性\n",
    "contents|第一个该标签下的所有内容，为列表形式\n",
    "string|第一个该标签或子标签下的文本内容，若标签中没有内容或者超过一层的子标签，则返回 None\n",
    "text|第一个该标签下（包括子标签）的所有文本内容，若没有内容则返回空文本\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "085f7bb6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.p.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd56edc0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'lh'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.p.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd94f119",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " <a href=\"http://home.baidu.com\">关于百度</a>,\n",
       " ' ',\n",
       " <a href=\"http://ir.baidu.com\">About Baidu</a>,\n",
       " ' ']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.p.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "292c9922",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "soup.p.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7c6fb91",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 关于百度 About Baidu '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.p.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7542490",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "将挖网页内容传递给 BeautifulSoup 提取时，还可以指定解析器，例如 'html.parser', 'lxml', 'xml', 'html5lib'。这些解析器各有利弊，其中 'lxml' 解析速度最快。将上面的 html_doc 的网页文件传递给 BeautifulSoup："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d090ed3a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "soup2 = BeautifulSoup(html_doc, 'lxml') # 或者 soup2 = BeautifulSoup(html_doc, features = 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2cb7ff5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLink 1\\nLink 2\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.ul.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e52c8990",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(soup2.ul.string) # 文本内容嵌套超过一层，返回 None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63987bd1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"c1\" href=\"http://example1.com\">Link 1</a>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.li.contents # html 语言中 a 标签是超链接，ul 与 li 标签表示一个无序列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74d5f6dc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Link 1'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.ul.li.text # 标签通过逗点套子标签"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4460017d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 函数`findall()`，`find()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c4da61",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "当需要列出同一类标签对应的所有内容时，需要用到 BeautifulSoup 中的 findall() 函数。\n",
    "\n",
    "<table>   \n",
    "    <tr style=\"border-top:solid; border-bottom:solid\">\n",
    "            <th colspan=2 style=\"text-align:center\">find_all([name], [attrs], [string], [limit], **kwargs)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">name</td>\n",
    "        <td style=\"text-align:left\">标签名字</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">attrs</td>\n",
    "        <td style=\"text-align:left\">按照标签的具体属性检索, 采用 class_= 形式或 JSON 格式等</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">string</td>\n",
    "        <td style=\"text-align:left\">按照关键字检索，采用 string= 形式，返回与关键字完全匹配的字符串</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">limit</td>\n",
    "        <td style=\"text-align:left\">返回结果的个数，默认返回所有结果</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom:solid\">\n",
    "        <td style=\"text-align:left\">**kwargs</td>\n",
    "        <td style=\"text-align:left\">其他参数</td>\n",
    "    </tr>\n",
    "</table>\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35770fef",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "还有一个函数`find()`，与`findall()`的区别在于`find()`只寻找第一个对应标签的内容。还有一个函数`find_next()`，可以查找标签的下一个标签。类似的检索函数还有`select`，本书不再赘述。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "124d9872",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"c1\" href=\"http://example1.com\">Link 1</a>,\n",
       " <a class=\"c2\" href=\"http://example2.org\">Link 2</a>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.find_all('a') # 检索出网页中所有的 a 标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a910cd1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"c1\" href=\"http://example1.com\">Link 1</a>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.find_all('a', class_ = 'c1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c94f4d80",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"c1\" href=\"http://example1.com\">Link 1</a>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.find_all(class_ = 'c1') # 参数中可以没有标签名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24cab8f0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.find_all(string = 'Link') # 关键词检索"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f9d54f",
   "metadata": {},
   "source": [
    "### 函数`get()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539042c2",
   "metadata": {},
   "source": [
    "通过函数`get`可以访问标签的具体属性，例如 class 类型，具体超链接等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47db6246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c1']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.a.get('class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e52b7cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://example1.com'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.a.get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e8d4a4f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Link 1', 'Link 2']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re # 导入正则表达式库\n",
    "\n",
    "soup2.find_all(string = re.compile('Link')) # 通过正则表达式将所有包含 Link 的字符串都检索出来"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d7d8a8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "````{note}\n",
    "正则表达式是计算机科学中用来描述字符串的一种表达式。正则表达式定义一个字符串表达规则，只要字符串满足这个规则，就算则匹配。可以通过字符串结合符号 .*?<=()[]{}\\d\\w 等定义多种表达规则，并结合函数`findall()`,`search()`,`match()`等检索字符串，具体可以进一步查阅相关资料。\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16906a4",
   "metadata": {},
   "source": [
    "正则表达式中：\n",
    "- . 匹配任意一个字符\n",
    "- \\* 匹配 0 个或多个表达式\n",
    "- \\+ 匹配 1 个或多个表达式\n",
    "- ? 匹配 0 个或1个前面定义的正则表达式片段，非贪婪方式\n",
    "- \\s 表示匹配任意空白字符\n",
    "- \\S 表示匹配任意非空字符\n",
    "- \\d 表示匹配任意数字\n",
    "- -\\D 表示匹配任意非数字\n",
    "- ^ 匹配一行字符串的开头\n",
    "- {n} 精确匹配前面的 n 个表达式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fd5c65",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "BeautifulSoup 另外一个常用的获取文本内容的函数为`get_text`。与直接使用标签的属性 text 相比，`get_text` 更加灵活，能够方便地实现文本换行。它的一般用法为：\n",
    "\n",
    "<table>   \n",
    "    <tr style=\"border-top:solid; border-bottom:solid\">\n",
    "            <th colspan=2 style=\"text-align:center\">get_text([separator=''], [strip=False], **kwargs)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">separator</td>\n",
    "        <td style=\"text-align:left\">各标签下文本的分隔符</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">strip</td>\n",
    "        <td style=\"text-align:left\">是否去掉各标签文本的前后空格</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom:solid\">\n",
    "        <td style=\"text-align:left\">**kwargs</td>\n",
    "        <td style=\"text-align:left\">其他参数</td>\n",
    "    </tr>\n",
    "</table>\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e17f7374",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntext1 text2\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.div.text # html 语言中 div 标签表示网页中的一个区域"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d345bab2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntext1 text2\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.div.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20adcbd8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ntext1\\n \\ntext2\\n\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.div.get_text(separator = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86e3fb0f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n**text1** **text2**\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.div.get_text(separator = '**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb2ff8ec",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text1text2'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.div.get_text(strip = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44b1f1a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 爬虫实例 1：抓小说\n",
    "\n",
    "下面，我们举例如何用爬虫抓取网上的《三国演义》小说，并将每回的内容下载到电脑里。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c3e80d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "首先，找到一个包含《三国演义》小说的网站，本例中，使用了古诗文网：https://so.gushiwen.cn/guwen/book_46653FD803893E4F7F702BCF1F7CCE17.aspx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ea2fad",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "然后，在网页的空白处，单击鼠标右键，查看网页源代码：\n",
    "\n",
    "![san_guo1](_build/html/_images/san_guo1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f40a20",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "网页的源代码大致如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8506f000",
   "metadata": {},
   "source": [
    "![sanguo_2.png](_build/html/_images/san_guo2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936a4bee",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- 我们发现每一回的链接在网页中的标签 ul 里面，具体在其每个子标签 span 中的子标签 a 里面。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33113c0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "打开其中任一回的链接，在网页空白处单击右键，查看网页源代码：\n",
    "\n",
    "![sanguo_3.png](_build/html/_images/san_guo3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a98aa9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 我们发现每一回的内容在具体属性为 class='contson' 的 div 标签里，其中第一句为每一回的标题，而之后的内容为正文。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f620926",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "根据网页的以上两个特点，设计 Python 的爬虫代码如下。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b66b3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```{code-block}python\n",
    "import requests  # 联系网络的包，a package for requesting from websites\n",
    "from bs4 import BeautifulSoup  # 分析网页数据的包，a package for webstie data analysis\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "# 获取单个网页信息\n",
    "def get_url_content(url):\n",
    "    # headers = {\n",
    "    #     'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.108 Safari/537.36'\n",
    "    # }  # 主要是模拟浏览器登录，防止反爬，一般的网站没有这个也行，谷歌浏览器 按 F12 能够找到网站支持的 headers\n",
    "    r = requests.get(url, timeout=30)  # 获取网页的内容，并返回给r变量，timeout 为超时时间\n",
    "    # r.raise_for_status()  # 检查返回网页的内容的状态码，200表示成功\n",
    "    # r.encoding = r.apparent_encoding  # 统一编码方式\n",
    "    return r.text  # 返回网页中的文本内容，数据内容为 r.content\n",
    "\n",
    "\n",
    "# 解析出网页中想要的信息，爬虫的关键步骤\n",
    "def filter_info(chapter_num, url_text):\n",
    "    soup = BeautifulSoup(url_text, \"lxml\")  # 解析网页返回内容，lxml 是一个解码方式，效率比较快，被推荐使用\n",
    "    contents = soup.find('div', class_='contson')\n",
    "    # 使用 get_text 可以实现网页里面 p 标签中文本内容的换行，而 text 不能\n",
    "    chapter_title = contents.find('p').get_text()  # 第一句话是本回的标题\n",
    "    chapter_title = '第' + str(chapter_num) + '回 ' + chapter_title.lstrip()\n",
    "    # chapter_content = contents.get_text(separator=\"\\n\")  # find 返回的不是列表，不用跟 [0]\n",
    "    chapter_content = contents.text\n",
    "    chapter_content = chapter_content.lstrip() # 去掉字符串左边的空字符\n",
    "    this_chapter = [chapter_title, chapter_content]\n",
    "    return this_chapter\n",
    "\n",
    "# 从网页中找到每一回的链接\n",
    "def get_url_links(url_head_content):\n",
    "    soup = BeautifulSoup(url_head_content, \"lxml\")  # 解析网页返回内容，lxml 是一个解码方式，效率比较快，被推荐使用\n",
    "    # links = soup.find('div', class_='bookcont')  # 每一回的链接都在类 span 里面\n",
    "    links = soup.find('ul')  # 每一回的链接都在类 ul 里面\n",
    "    links = links.findAll('span')\n",
    "    link_list = []\n",
    "    for each in links:\n",
    "        link_list.append(each.a.get('href'))  # 获取每一回的链接，存储到列表里\n",
    "    return link_list\n",
    "\n",
    "# 将每章内容输出到 txt 文档里\n",
    "def write_txt(string_array):\n",
    "    file_address = 'E:/三国演义/'  # txt 存放地址\n",
    "    if not os.path.exists(file_address):       #用os库判断对应文件夹是否存在\n",
    "        os.makedirs(file_address)              #如果没有对应文件夹则自动生成 \n",
    "    file_name = string_array[0]\n",
    "    with open(file_address + file_name + '.txt', 'w', encoding='utf-8') as f: # 必须跟解码形式，不然有的网页中文内容写不到txt里\n",
    "        f.write(string_array[1])\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    url = 'https://so.gushiwen.cn/gushi/tangshi.aspx'  # 古诗文网三国演义网址\n",
    "    url_head_content = get_url_content(url)  # 获取网页\n",
    "    links = get_url_links(url_head_content)  # 获取每一回的链接地址\n",
    "    # enumerate() 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列\n",
    "    # 同时列出数据下标和数据\n",
    "    for index, each in enumerate(links):\n",
    "        url_link_content = get_url_content(each)  # 获取每一回的网页内容\n",
    "        chapter_content = filter_info(index + 1, url_link_content)  # 解析每一回的网页内容，获取小说文本\n",
    "        write_txt(chapter_content)  # 输出小说内容到 txt\n",
    "        time.sleep(random.random() * 1)  # 每抓一个网页休息0~1秒，防止被反爬措施封锁 IP\n",
    "\n",
    "# 运行函数\n",
    "main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9c3386",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "其中，使用 requests 库时，有的网站需要设置 headers 参数才能正确模拟浏览器登录。headers 参数可以在打开网页时，按键盘`F12` 打开， 然后刷新一下网页，单击`F12`弹出窗口的任何一个文件，就能查看网站的所有 headers 信息，有 cookie，user-agent 等信息。如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e89bc7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![sanguo_4.png](_build/html/_images/san_guo4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2d6563",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "我们可以复制一部分信息或全部信息，放到 request 函数的 headers 参数里面。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fa33ca",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 爬虫实例 2：抓豆瓣 top 250 的电影信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefbd4a2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "我们想获取豆瓣高分评价的 250 部电影的信息：电影名字，导演，上映年代，评分，评分人数等。打开豆瓣 top 250 的网页链接 https://movie.douban.com/top250?start= ，发现每个网页显示 25 部电影，一共有 10 个挖网页，网页地址与链接中的 start = 后面的数字有关。例如第 3 个子网页的链接地址是 'https://movie.douban.com/top250?start=50 ，根据这个规律，我们可以用一个 for 循环，将 10 个网页分别打开抓取信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ae712e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "打开一个网页，单击右键查看网页源代码，如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cf43e6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![douban1.png](_build/html/_images/douban1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a0a878",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "通过源代码发现以下几个特点：\n",
    "\n",
    "- 每部电影的信息都在标签 &lt;div class='item'&gt; 里面。据此，我们可以用一个 for 循环，将每个网页的  &lt;div class='item'&gt; 标签都检索出来\n",
    "\n",
    "- 电影的中英文名字都在标签  &lt;div class='title'&gt; 里，而其他名字在标签 &lt;div class='other'&gt; 里。我们可以使用函数`find()`以及`find_next()` 检索出来。\n",
    "\n",
    "- 导演，主演，上映年代，电影产地，电影类型信息在标签 &lt;p class=''&gt; 里。我们可以使用函数`find()`检索，然后利用正则表达式将里面对应文本信息分别提取出来。\n",
    "\n",
    "- 评分信息在标签 &lt;div class='rating_num'&gt; 里。我们可以使用函数`find()`检索出来。\n",
    "\n",
    "- 评分人数信息在标签 &lt;span&gt; 里。我们可以通过函数`find()`结合关键字'评价'检索出来。（网页中的 &lt; span &gt; 提供了一种将文本的一部分或者文档的一部分独立出来的方式）\n",
    "\n",
    "- 代表性评价信息在标签 &lt;div class='quote'&gt; 里。我们可以使用函数`find()`检索出来。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472e3e85",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "根据以上的特点，设计出以下的 python 代码。其中中文分词库`jieba`，词云库`WordCloud`库等，若没有安装，需要提前安装（使用 pip 或 conda 安装）到电脑上。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6651abe3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "在画图时，使用 matplotlib 画出饼图，使用 WordCloud 生成词云图，因为 matplotlib 不能自动将不同柱子分配不同的颜色，使用 seaborn 画出柱状图。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45450fad",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```{code-block}python\n",
    "import requests  # 联系网络的包，a package for requesting from websites\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup # 分析网页数据的包，a package for webstie data analysis\n",
    "import matplotlib.pyplot as plt # 画图的包\n",
    "import time \n",
    "import random\n",
    "import jieba # 中文分词包\n",
    "from wordcloud import WordCloud # 词云包\n",
    "import re  # 正则表达式包\n",
    "import seaborn as sns # 用 seaborn 画柱状图\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei'] \n",
    "\n",
    "# 主要是模拟浏览器登录，防止反爬\n",
    "headers = {\n",
    "    'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.108 Safari/537.36'\n",
    "    }\n",
    "\n",
    "## store the information\n",
    "rank_list = []\n",
    "name_list = [] # 用来存储电影名字的一个二维列表\n",
    "director_list = [] # 用来存储导演名字的一个列表，下面几个变量类似\n",
    "actor_list = []\n",
    "year_list = []\n",
    "rating_list = []\n",
    "ratingNum_list = []\n",
    "quote_list = []\n",
    "place_list = []\n",
    "category_list = []\n",
    "\n",
    "num = 0\n",
    "for i in range(0, 10):\n",
    "    link = 'https://movie.douban.com/top250?start=' + str(i*25) # 250部电影一共 10个网页， 10 pages for total 250 movies    \n",
    "    res = requests.get(link, headers = headers, timeout = 10)\n",
    "    time.sleep(random.random()*3) # 每抓一个网页休息2~3秒，防止被反爬措施封锁 IP，avoid being blocked of the IP address\n",
    "    \n",
    "    # res.text is the content of the crawler\n",
    "    soup = BeautifulSoup(res.text, \"lxml\")  # lxml 是一个解码方式，lxml is one decoding model for Beautifulsoup\n",
    "    movie_list = soup.find_all('div', class_ = 'item') # 检索所有类型为 item 的 div 标签\n",
    "    \n",
    "    for each in movie_list:\n",
    "        # 排名在 em 标签里\n",
    "        rank = each.em.text\n",
    "        rank_list.append(rank)\n",
    "        \n",
    "        # 电影名字在标签 <span class=\"title\"> 与 <span class=\"other\"> 里\n",
    "        other_name = each.find('span', class_ = 'other').get_text()\n",
    "        other_name = other_name.replace(' ', '') # 去掉空格\n",
    "        other_name = other_name.replace('\\xa0/\\xa0', '') # 去掉多余字符 \\xa0/\\xa0\n",
    "        cn_name = each.find('span', class_ = 'title').get_text()\n",
    "        eg_name = each.find('span', class_ = 'title').find_next().get_text() # find_next() 查找满足条件的下一个标签\n",
    "        eg_name = eg_name.replace('\\xa0/\\xa0', '') # 去掉多余字符 \\xa0/\\xa0\n",
    "        name_list.append([cn_name, eg_name, other_name])   \n",
    "              \n",
    "        # 评分信息在标签  <span class=\"rating_num\"> 里\n",
    "        rating =  each.find('span', class_ = 'rating_num').get_text()\n",
    "        rating_list.append(float(rating))\n",
    "        \n",
    "        # 评价人数通过关键词'评价'检索\n",
    "        rating_num = each.find(string = re.compile('评价')).get_text()\n",
    "        rating_num = rating_num.replace('人评价', '')\n",
    "        ratingNum_list.append(int(rating_num))\n",
    "        \n",
    "        # 代表性评价在  <p class=\"quote\"> 里  \n",
    "        try:\n",
    "            quote =  each.find('p', class_ = 'quote').get_text()\n",
    "        except Exception: # 有的电影代表性评价没有显示\n",
    "            quote =  ''\n",
    "        quote_list.append(quote)\n",
    "        \n",
    "        info = each.p.get_text(strip = True)\n",
    "        # 定义正则表达式提取出导演，主演，上映时间，地点，电影类型信息        \n",
    "        try: \n",
    "            # (?<=导演: ) match any character begins after character '导演: '\n",
    "            # .*? match any character (.), zero or more times (*) but as less as possible (?)\n",
    "            # (?=主) match any character before character '主'\n",
    "            director = re.compile('(?<=导演: ).*?(?=主)').findall(info)[0]\n",
    "            actor = re.compile('(?<=主演: ).*?(?=/)').findall(info)[0]\n",
    "        except Exception: # 有的电影导演名字太长，主演没有显示出来\n",
    "            director = re.compile('(?<=导演: ).*?(?=\\xa0)').findall(info)[0]\n",
    "            actor = ''  \n",
    "        director_list.append(director)\n",
    "        actor_list.append(actor)\n",
    "        \n",
    "        # \\d{4} is a four digit number\n",
    "        year = re.compile('(?<=...)\\d{4}').findall(info)[0]\n",
    "        year_list.append(year)\n",
    "        place_category = re.compile('(?<=\\xa0/\\xa0).*').findall(info)[0]\n",
    "        place_category = place_category.replace('\\xa0', '')\n",
    "        place_category = place_category.split('/')\n",
    "        place = place_category[0]\n",
    "        category = place_category[1]\n",
    "        place_list.append(place)\n",
    "        category_list.append(category)\n",
    "          \n",
    "# 将数据存到 pandas 里\n",
    "df = pd.DataFrame(rank_list, columns = ['排名'])    \n",
    "df['电影名字'] = [i[0] for i in name_list]\n",
    "df['外文名字'] = [i[1] for i in name_list]\n",
    "df['其他名字'] = [i[2] for i in name_list]\n",
    "df['评分'] = rating_list\n",
    "df['评价人数'] = ratingNum_list\n",
    "df['导演'] = director_list\n",
    "df['主演'] = actor_list\n",
    "df['上映日期'] = year_list\n",
    "df['地区'] = place_list\n",
    "df['类型'] = category_list\n",
    "df['代表性评论'] = quote_list\n",
    "\n",
    "# 导出到 xls 文件里，save to xls file    \n",
    "df.to_csv('豆瓣 top 250 电影爬虫抓取.csv')\n",
    "\n",
    "# 分析电影来源地并画饼图\n",
    "locations = []\n",
    "for i in range(len(place_list)):\n",
    "    nations = place_list[i].split(' ') \n",
    "    for j in range(len(nations)):\n",
    "        if nations[j] == '西德':\n",
    "            nations[j] = '德国'\n",
    "        locations.append(nations[j])\n",
    "\n",
    "df_location = pd.DataFrame(locations, columns = ['地区'])\n",
    "# 按照出现次数排序, size() 可以查数，生成一个 Series 类型\n",
    "# 然后用 reset_index 重新命名，参数为 name\n",
    "# DataFrame 类型的 reset_index 参数为 names\n",
    "df2 = df_location.groupby('地区').size().reset_index(name='counts') # df2 = df2['地区'].value_counts(ascending = False).reset_index()\n",
    "df2.sort_values(by = 'counts', ascending = False, inplace = True, ignore_index = True)\n",
    "# 画饼状图\n",
    "values = []\n",
    "labels = []\n",
    "other_count = 0\n",
    "for i in range(9):\n",
    "    values.append(df2['counts'][i])\n",
    "    labels.append(df2['地区'][i])\n",
    "for i in range(9, df2.shape[0]):\n",
    "    other_count += int(df2['counts'][i])\n",
    "values.append(other_count)\n",
    "labels.append('其他地区')\n",
    "plt.figure(1)\n",
    "plt.pie(values, labels=labels, autopct='%.2f%%')\n",
    "plt.legend()  # 显示标签\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 分析电影类型并画饼图\n",
    "categories = []\n",
    "for i in range(len(category_list)):\n",
    "    category = category_list[i].split(' ') \n",
    "    for j in range(len(category)):\n",
    "        categories.append(category[j])\n",
    "        \n",
    "df_category = pd.DataFrame(categories, columns = ['类型'])    \n",
    "df3 = df_category.groupby('类型').size().reset_index(name='counts') \n",
    "df3.sort_values(by = 'counts', ascending = False, inplace = True, ignore_index = True)\n",
    "\n",
    "values = []\n",
    "labels = []\n",
    "other_count = 0\n",
    "for i in range(15):\n",
    "    values.append(df3['counts'][i])\n",
    "    labels.append(df3['类型'][i])\n",
    "for i in range(15, df3.shape[0]):\n",
    "    other_count += int(df3['counts'][i])\n",
    "values.append(other_count)\n",
    "labels.append('其他类型')\n",
    "plt.figure(2)\n",
    "plt.pie(values, labels=labels, autopct='%.2f%%')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 画词云图\n",
    "jieba.add_word('久石让')\n",
    "jieba.add_word('谢耳朵')\n",
    "# 一些语气词和没有意义的词\n",
    "del_words = [ '就是', '一个', '被', '电影', '我们',\n",
    "              '不是', '每个',  '不会',  '没有', \n",
    "              '这样', '那么', '不要', '如果',\n",
    "              '不能',  '一种', '不过', '只有', '不得不', \n",
    "              '不得不', '一部']\n",
    "all_quotes = ''.join(quote_list) # 将所有代表性评论拼接为一个文本\n",
    "# 去掉标点符号\n",
    "all_quotes = re.sub(r\"[0-9\\s+\\.\\!\\/_,$%^*()?;；:-【】+\\\"\\']+|[+——！，;:。？、~@#￥%……&*（）]+\", \" \", all_quotes)\n",
    "words = jieba.lcut(all_quotes)\n",
    "words_final = []\n",
    "for i in range(len(words)): \n",
    "    if len(words[i]) > 1 and  words[i] not in del_words: # 去掉一些语气词，单字词 \n",
    "        words_final.append(words[i])\n",
    "text_result = Counter(words_final)\n",
    "cloud = WordCloud(\n",
    "    font_path = 'C:\\Windows\\Fonts\\FZSTK.TTF', # 中文字体地址 C:\\Windows\\Fonts\\FZSTK.TTF，提前下载字体或指定，否则中文无法显示\n",
    "    # mask = '' 通过 mask 参数指定一个图片地址作为词云的背景图像 \n",
    "    background_color = 'white',\n",
    "    width = 1000,\n",
    "    height = 860,\n",
    "    max_words = 25   \n",
    "  )\n",
    "#wc = cloud.generate(words) # 这种方法对中文支持不太好，this mehtod is better for only english string\n",
    "wc = cloud.generate_from_frequencies(text_result)\n",
    "wc.to_file(\"豆瓣 TOP 250 词云.jpg\") \n",
    "plt.figure(3)\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.title('豆瓣 TOP 250 电影代表性评论的词云分析')\n",
    "plt.show()\n",
    "\n",
    "# 评分最高的 15 部电影\n",
    "# 用 seaborn 画柱状图，因为自动将不同柱子分配不同的颜色\n",
    "df_star = df.sort_values(by = '评分', ascending = False, ignore_index = True)\n",
    "number = 15\n",
    "df_star_top = df_star.head(number)\n",
    "plt.figure(4)\n",
    "sns.barplot(data = df_star_top, y = '电影名字', x = '评分', orient = 'h')\n",
    "plt.title('评分最高的 ' + str(number) + ' 部电影')\n",
    "plt.show()\n",
    "\n",
    "# 评分人数最多的 15 部电影\n",
    "df_num = df.sort_values(by = '评价人数', ascending = False, ignore_index = True)\n",
    "df_num_top = df_num.head(number)\n",
    "plt.figure(5)\n",
    "sns.barplot(data = df_num_top, y = '电影名字', x = '评价人数', orient = 'h')\n",
    "plt.title('评分人数最多的 ' + str(number) + ' 部电影')\n",
    "plt.show()\n",
    "\n",
    "# 电影年代画图\n",
    "df_year = df.groupby('上映日期').size().reset_index(name = '部数')\n",
    "df_year.sort_values('部数', ascending = False, inplace = True)\n",
    "df_year_top = df_year.head(20)\n",
    "df_year_top.sort_values('上映日期', inplace = True)\n",
    "plt.figure(6)\n",
    "sns.barplot(data = df_year_top, x = '上映日期', y = '部数')\n",
    "plt.title('电影上映年代')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941f40d7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "生成的图形如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9699ad1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![douban2.png](_build/html/_images/douban2.png)\n",
    "![douban3.png](_build/html/_images/douban3.png)\n",
    "![douban4.png](_build/html/_images/douban4.png)\n",
    "![douban5.png](_build/html/_images/douban5.png)\n",
    "![douban6.png](_build/html/_images/douban6.png)\n",
    "![douban7.png](_build/html/_images/douban7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f015cdae",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 后记\n",
    "\n",
    "- 不同的网站的爬虫技巧经常不一样，要具体网站具体分析。\n",
    "- 由于一些网站经常更新数据的存储地址或源代码，某段时间的爬虫代码可能在下一个时间段就不能用了，爬虫代码也要更新。\n",
    "- 有的网页信息并不在网站直接打开的网页源代码里（例如弹幕、电商评论等），还要结合按键`F12`找到网页信息的真实地址。\n",
    "- 提取网页信息时，经常要用到正则表达式处理字符串。\n",
    "\n",
    "\n",
    "近年来，许多网站都提高了反爬手段防止网站的数据被无端获取。因此，提醒读者避免过度使用爬虫，给一些网站造成不必要的访问负担。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a15a83d",
   "metadata": {},
   "source": [
    "## 练习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7cac8e",
   "metadata": {},
   "source": [
    "```{exercise}\n",
    ":label: crab-poem\n",
    "练习从网上抓取一首诗词。\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fbf5b8",
   "metadata": {},
   "source": [
    "```{exercise}\n",
    ":label: crab-news\n",
    "练习抓取学校的新闻，并尝试画出词云图。\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85dded6",
   "metadata": {},
   "source": [
    "```{exercise}\n",
    ":label: crab-红楼\n",
    "尝试用爬虫下载小说《红楼梦》到电脑里。\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "幻灯片",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}