{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2ef5959",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 爬虫入门\n",
    "\n",
    "\n",
    "Python 的爬虫功能使得程序员可以快速抓取并分析网页中的信息，它实质上是模拟浏览器访问网页。本章主要常用的两个爬虫相关的库`requests`，`beautifulsoup4`。若要模拟鼠标点击等，要用到`selenium` 库，限于篇幅限制，本章不再介绍，具体可以查阅相关资料。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbbb767",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## `requests` 库\n",
    "\n",
    "\n",
    "每个网页都有源代码，可以通过鼠标单击右键查看网页源代码。网页中的很多信息都在源代码里面，`requests` 是一个访问网页源代码的库。一般通过 get 函数访问网页，另外一个常用来访问网页的函数是 post，与 get 函数的区别在于 post 能够传递表格或文件到网页所在服务器上。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d386cb6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table>   \n",
    "    <tr style=\"border-top:solid; border-bottom:solid\">\n",
    "            <th colspan=2 style=\"text-align:center\">get(url, [timeout], [headers], [proxies], **kwargs)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">url</td>\n",
    "        <td style=\"text-align:left\">网页链接</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">timeout</td>\n",
    "        <td style=\"text-align:left\">可选参数，请求网页的最大时长，单位为秒</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">headers</td>\n",
    "        <td style=\"text-align:left\">可选参数，模拟浏览器设置</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">proxies</td>\n",
    "        <td style=\"text-align:left\">可选参数，代理服务器设置</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom:solid\">\n",
    "        <td style=\"text-align:left\">**kwargs</td>\n",
    "        <td style=\"text-align:left\">其他参数</td>\n",
    "    </tr>\n",
    "</table>\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b589f11f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "get 或 post 函数返回一个 Resoponse 对象，该对象包括以下常用的属性或函数。\n",
    "\n",
    "|属性或函数|描述|\n",
    "|:--|:--|\n",
    "status_code|网页请求的返回状态，200 表示连接成功，404 表示连接失败|\n",
    "text|响应网页的字符串内容\n",
    "encoding|响应网页的编码方式，可以更改\n",
    "content|相应网页的字节形式内容，例如图片或 pdf 文件等\n",
    "raise_for_status()| 如果网页访问不成功，抛出异常，一般结合 try-except 语句使用\n",
    "json() | 该函数可以解析网页内容中 JSON 格式的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0edd6555",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get(\"http://www.baidu.com\")\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873e6bef",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```{code-block} python\n",
    "r.text # 结果省略\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fddc805f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ISO-8859-1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.encoding  # 若为 ISO-8859-1 则中文为乱码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "099932fe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "r.encoding = \"utf-8\"  # 无论网页原内容是什么编码，都改成 utf-8 编码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e46dba7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## `beautifulsoup4`库\n",
    "\n",
    "\n",
    "使用`requests`获取的网页源代码一般非常复杂，不仅包括常规内容，还包括很多定义页面格式的代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f4c6db",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`````{admonition} tips\n",
    ":class: tip\n",
    "- 网页中的内容一般在网页源代码的各个标签里\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4c09d4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "例如有下面的 html 代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faee552e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "html_doc = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "        <h1>Hello, BeautifulSoup!</h1>\n",
    "        <ul>\n",
    "            <li><a class = \"c1\" href=\"http://example1.com\">Link 1</a></li>\n",
    "            <li><a class = \"c2\" href=\"http://example2.org\">Link 2</a></li>\n",
    "        </ul>\n",
    "        <div>\n",
    "        <p>text1</p> <p>text2</p>\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac7df72",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`beautifulsoup4`库提供了大量的属性或函数，能够方便地将网页（html）不同标签（tag）中的内容提取出来。常用的属性有下面几个："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae03272",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "|属性|描述|\n",
    "|:--|:--|\n",
    "head|网页源代码中的 &lt;head&gt; 标签内容\n",
    "title|网页源代码中的 &lt;title&gt; 标签内容\n",
    "body|网页源代码中的 &lt;body&gt; 标签内容\n",
    "p|网页源代码中的第一个 &lt;p&gt; 标签内容\n",
    "a|网页源代码中的第一个 &lt;a&gt; 标签内容\n",
    "div| 网页源代码中的第一个 &lt;div&gt; 标签内容\n",
    "script| 网页源代码中的第一个 &lt;script&gt; 标签内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "680f928c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "r = requests.get(\"http://www.baidu.com\")\n",
    "r.encoding = \"utf-8\"\n",
    "soup = BeautifulSoup(r.text)  # 将网页内容传递给 BeautifulSoup 提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c0995f8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>百度一下，你就知道</title>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d2a5dff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p id=\"lh\"> <a href=\"http://home.baidu.com\">关于百度</a> <a href=\"http://ir.baidu.com\">About Baidu</a> </p>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.p  # html 语言中 p 标签表示一个文本段落"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1dc8fb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "上面的这些标签对象还有自己的属性可以访问更具体的内容。\n",
    "\n",
    "|标签属性|描述|\n",
    "|:--|:--|\n",
    "name|标签名字，例如 a，p，div等\n",
    "attrs|标签的具体属性\n",
    "contents|第一个该标签下的所有内容，为列表形式\n",
    "string|第一个该标签或子标签下的文本内容，若标签中没有内容或者超过一层的子标签，则返回 None\n",
    "text|第一个该标签下（包括子标签）的所有文本内容，若没有内容则返回空文本\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "085f7bb6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.p.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd56edc0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'lh'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.p.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd94f119",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " <a href=\"http://home.baidu.com\">关于百度</a>,\n",
       " ' ',\n",
       " <a href=\"http://ir.baidu.com\">About Baidu</a>,\n",
       " ' ']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.p.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "292c9922",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "soup.p.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7c6fb91",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 关于百度 About Baidu '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.p.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7542490",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "将挖网页内容传递给 BeautifulSoup 提取时，还可以指定解析器，例如 'html.parser', 'lxml', 'xml', 'html5lib'。这些解析器各有利弊，其中 'lxml' 解析速度最快。将上面的 html_doc 的网页文件传递给 BeautifulSoup："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d090ed3a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "soup2 = BeautifulSoup(\n",
    "    html_doc, \"lxml\"\n",
    ")  # 或者 soup2 = BeautifulSoup(html_doc, features = 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2cb7ff5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLink 1\\nLink 2\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.ul.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e52c8990",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(soup2.ul.string)  # 文本内容嵌套超过一层，返回 None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63987bd1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"c1\" href=\"http://example1.com\">Link 1</a>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.li.contents  # html 语言中 a 标签是超链接，ul 与 li 标签表示一个无序列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74d5f6dc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Link 1'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.ul.li.text  # 标签通过逗点套子标签"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4460017d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 函数`findall()`，`find()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c4da61",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "当需要列出同一类标签对应的所有内容时，需要用到 BeautifulSoup 中的 findall() 函数。\n",
    "\n",
    "<table>   \n",
    "    <tr style=\"border-top:solid; border-bottom:solid\">\n",
    "            <th colspan=2 style=\"text-align:center\">find_all([name], [attrs], [string], [limit], **kwargs)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">name</td>\n",
    "        <td style=\"text-align:left\">标签名字</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">attrs</td>\n",
    "        <td style=\"text-align:left\">按照标签的具体属性检索, 采用 class_= 形式或 JSON 格式等</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">string</td>\n",
    "        <td style=\"text-align:left\">按照关键字检索，采用 string= 形式，返回与关键字完全匹配的字符串</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">limit</td>\n",
    "        <td style=\"text-align:left\">返回结果的个数，默认返回所有结果</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom:solid\">\n",
    "        <td style=\"text-align:left\">**kwargs</td>\n",
    "        <td style=\"text-align:left\">其他参数</td>\n",
    "    </tr>\n",
    "</table>\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35770fef",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "还有一个函数`find()`，与`findall()`的区别在于`find()`只寻找第一个对应标签的内容。还有一个函数`find_next()`，可以查找标签的下一个标签。类似的检索函数还有`select`，本书不再赘述。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "124d9872",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"c1\" href=\"http://example1.com\">Link 1</a>,\n",
       " <a class=\"c2\" href=\"http://example2.org\">Link 2</a>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.find_all(\"a\")  # 检索出网页中所有的 a 标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a910cd1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"c1\" href=\"http://example1.com\">Link 1</a>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.find_all(\"a\", class_=\"c1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c94f4d80",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"c1\" href=\"http://example1.com\">Link 1</a>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.find_all(class_=\"c1\")  # 参数中可以没有标签名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24cab8f0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.find_all(string=\"Link\")  # 关键词检索"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f9d54f",
   "metadata": {},
   "source": [
    "### 函数`get()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539042c2",
   "metadata": {},
   "source": [
    "通过函数`get`可以访问标签的具体属性，例如 class 类型，具体超链接等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47db6246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c1']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.a.get(\"class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e52b7cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://example1.com'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.a.get(\"href\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e8d4a4f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Link 1', 'Link 2']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re  # 导入正则表达式库\n",
    "\n",
    "soup2.find_all(string=re.compile(\"Link\"))  # 通过正则表达式将所有包含 Link 的字符串都检索出来"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d7d8a8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "````{note}\n",
    "正则表达式是计算机科学中用来描述字符串的一种表达式。正则表达式定义一个字符串表达规则，只要字符串满足这个规则，就算则匹配。可以通过字符串结合符号 .*?<=()[]{}\\d\\w 等定义多种表达规则，并结合函数`findall()`,`search()`,`match()`等检索字符串，具体可以进一步查阅相关资料。\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fd5c65",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "BeautifulSoup 另外一个常用的获取文本内容的函数为`get_text`。与直接使用标签的属性 text 相比，`get_text` 更加灵活，能够方便地实现文本换行。它的一般用法为：\n",
    "\n",
    "<table>   \n",
    "    <tr style=\"border-top:solid; border-bottom:solid\">\n",
    "            <th colspan=2 style=\"text-align:center\">get_text([separator=''], [strip=False], **kwargs)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">separator</td>\n",
    "        <td style=\"text-align:left\">各标签下文本的分隔符</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">strip</td>\n",
    "        <td style=\"text-align:left\">是否去掉各标签文本的前后空格</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom:solid\">\n",
    "        <td style=\"text-align:left\">**kwargs</td>\n",
    "        <td style=\"text-align:left\">其他参数</td>\n",
    "    </tr>\n",
    "</table>\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e17f7374",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntext1 text2\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.div.text  # html 语言中 div 标签表示网页中的一个区域"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d345bab2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntext1 text2\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.div.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20adcbd8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ntext1\\n \\ntext2\\n\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.div.get_text(separator=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86e3fb0f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n**text1** **text2**\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.div.get_text(separator=\"**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb2ff8ec",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text1text2'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.div.get_text(strip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cec5818",
   "metadata": {},
   "source": [
    "## 正则表达式\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8e8510",
   "metadata": {},
   "source": [
    "正则表达式是计算机科学中用来描述字符串的一种表达式。正则表达式定义一个字符串表达规则，只要字符串满足这个规则，就算匹配成功。可以通过字符串结合符号 .*?<=()[]{}\\d\\w 等定义多种表达规则，并结合函数`findall()`,`search()`,`match()`等检索字符串。通过正则表达式，我们可以实现更加复杂的文本检索。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ff0291",
   "metadata": {},
   "source": [
    "一些常见的正则表达式："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3789c3",
   "metadata": {},
   "source": [
    "|表达式|含义|\n",
    "|:--|:--|\n",
    "|.| 匹配任意一个字符 |\n",
    "| \\* |匹配 0 个或多个表达式|\n",
    "| \\+ |匹配 1 个或多个表达式|\n",
    "|.*| 匹配 0 个或多个字符 |\n",
    "|.+| 匹配 1 个或多个字符 |\n",
    "| ? |匹配 0 个 或 1 个前面的字符或正则表达式|\n",
    "| (?= ) |匹配但不包含等号后面的内容|\n",
    "| \\s| 表示匹配 0 个或多个空格|\n",
    "| \\s*| 表示匹配 1 个或多个空格|\n",
    "| \\s+| 表示匹配任意空格|\n",
    "|\\S |表示匹配任意非空字符|\n",
    "| \\d |表示匹配任意数字|\n",
    "|\\D |表示匹配任意非数字|\n",
    "| ^ |匹配一行字符串的开头|\n",
    "| ( ) |用来提取小括号内正则表达式匹配的内容|\n",
    "| [ ] |用来匹配中括号内所包含的任意一个字符，例如 [abc] 匹配 'a'，'b'或'c'|\n",
    "|{n} |精确匹配 n 个前面的表达式，例如 'o{2}'匹配 'food' 中的两个o|\n",
    "| {n, m} |匹配 n 到 m 次由前面的正则表达式|\n",
    "| \\b  |匹配单词边界|\n",
    "| \\w  |匹配字母/数字/下划线，不包括标点符号|\n",
    "| \\w*  |匹配 0 个或多个字母/数字/下划线|\n",
    "| \\w+  |匹配 1 个或多个字母/数字/下划线|\n",
    "|[A-Za-z0-9]+  |匹配 1 个或多个字母/数字/下划线|\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b80fe02",
   "metadata": {},
   "source": [
    "一些例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b400920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice.\n"
     ]
    }
   ],
   "source": [
    "text = \"My name is Alice.\"\n",
    "pattern = r\"My name is (.+)\"\n",
    "\n",
    "match = re.search(pattern, text)\n",
    "if match:\n",
    "    print(match.group(1))  # 输出: Alice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c16949b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice\n"
     ]
    }
   ],
   "source": [
    "text = \"My name is Alice.\"\n",
    "pattern = r\"My name is (\\w*)\"  # 不输出标吊符号\n",
    "\n",
    "match = re.search(pattern, text)\n",
    "if match:\n",
    "    print(match.group(1))  # 输出: Alice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79f264c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text1 = \"color\"\n",
    "text2 = \"colour\"\n",
    "pattern = r\"colou?r\"  # “u” 出现 0 次或 1 次\n",
    "\n",
    "print(bool(re.search(pattern, text1)))  # True\n",
    "print(bool(re.search(pattern, text2)))  # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35fc54f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'apple']\n"
     ]
    }
   ],
   "source": [
    "text = \"apple123 apple apple456\"\n",
    "pattern = r\"apple(?=\\d)\"  # 只匹配后面是数字的 \"apple\"，但不包含数字\n",
    "\n",
    "matches = re.findall(pattern, text)\n",
    "print(matches)  # 输出: ['apple', 'apple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04eaa2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ffood', 'food']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"fod and ffood are not food\"\n",
    "pattern = r\"\\b\\S*o{2}\\S*\\b\"  # 匹配包含至少两个 'o' 的单词，或 r'\\b\\w*o{2}\\S*\\w'\n",
    "\n",
    "matches = re.findall(pattern, text)\n",
    "print(matches)  # 输出: ['ffood', 'food']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44b1f1a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 爬虫实例 1：抓小说\n",
    "\n",
    "\n",
    "下面，我们举例如何用爬虫抓取网上的《三国演义》小说，并将每回的内容下载到电脑里。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c3e80d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "首先，找到一个包含《三国演义》小说的网站，本例中，使用了古诗文网：https://so.gushiwen.cn/guwen/book_46653FD803893E4F7F702BCF1F7CCE17.aspx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ea2fad",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "然后，在网页的空白处，单击鼠标右键，查看网页源代码：\n",
    "\n",
    "![san_guo1](_build/html/_images/san_guo1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f40a20",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "网页的源代码大致如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8506f000",
   "metadata": {},
   "source": [
    "![sanguo_2.png](_build/html/_images/san_guo2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936a4bee",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- 我们发现每一回的链接在网页中的标签 ul 里面，具体在其每个子标签 span 中的子标签 a 里面。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33113c0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "打开其中任一回的链接，在网页空白处单击右键，查看网页源代码：\n",
    "\n",
    "![sanguo_3.png](_build/html/_images/san_guo3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a98aa9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 我们发现每一回的内容在具体属性为 class='contson' 的 div 标签里，其中第一句为每一回的标题，而之后的内容为正文。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f620926",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "根据网页的以上两个特点，设计 Python 的爬虫代码如下。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b66b3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```{code-block} python3\n",
    "import requests  # 联系网络的包，a package for requesting from websites\n",
    "from bs4 import BeautifulSoup  # 分析网页数据的包，a package for webstie data analysis\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "# 获取单个网页信息\n",
    "def get_url_content(url):\n",
    "    # headers = {\n",
    "    #     'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.108 Safari/537.36'\n",
    "    # }  # 主要是模拟浏览器登录，防止反爬，一般的网站没有这个也行，谷歌浏览器 按 F12 能够找到网站支持的 headers\n",
    "    r = requests.get(url, timeout=30)  # 获取网页的内容，并返回给r变量，timeout 为超时时间\n",
    "    # r.raise_for_status()  # 检查返回网页的内容的状态码，200表示成功\n",
    "    # r.encoding = r.apparent_encoding  # 统一编码方式\n",
    "    return r.text  # 返回网页中的文本内容，数据内容为 r.content\n",
    "\n",
    "\n",
    "# 解析出网页中想要的信息，爬虫的关键步骤\n",
    "def filter_info(chapter_num, url_text):\n",
    "    soup = BeautifulSoup(url_text, \"lxml\")  # 解析网页返回内容，lxml 是一个解码方式，效率比较快，被推荐使用\n",
    "    contents = soup.find(\"div\", class_=\"contson\")\n",
    "    # 使用 get_text 可以实现网页里面 p 标签中文本内容的换行，而 text 不能\n",
    "    chapter_title = contents.find(\"p\").get_text()  # 第一句话是本回的标题\n",
    "    chapter_title = \"第\" + str(chapter_num) + \"回 \" + chapter_title.lstrip()\n",
    "    # chapter_content = contents.get_text(separator=\"\\n\")  # find 返回的不是列表，不用跟 [0]\n",
    "    chapter_content = contents.text\n",
    "    chapter_content = chapter_content.lstrip()  # 去掉字符串左边的空字符\n",
    "    this_chapter = [chapter_title, chapter_content]\n",
    "    return this_chapter\n",
    "\n",
    "\n",
    "# 从网页中找到每一回的链接\n",
    "def get_url_links(url_head_content):\n",
    "    soup = BeautifulSoup(url_head_content, \"lxml\")  # 解析网页返回内容，lxml 是一个解码方式，效率比较快，被推荐使用\n",
    "    # links = soup.find('div', class_='bookcont')  # 每一回的链接都在类 span 里面\n",
    "    links = soup.find(\"ul\")  # 每一回的链接都在类 ul 里面\n",
    "    links = links.findAll(\"span\")\n",
    "    link_list = []\n",
    "    for each in links:\n",
    "        link_list.append(each.a.get(\"href\"))  # 获取每一回的链接，存储到列表里\n",
    "    return link_list\n",
    "\n",
    "\n",
    "# 将每章内容输出到 txt 文档里\n",
    "def write_txt(string_array):\n",
    "    file_address = \"E:/三国演义/\"  # txt 存放地址\n",
    "    if not os.path.exists(file_address):  # 用os库判断对应文件夹是否存在\n",
    "        os.makedirs(file_address)  # 如果没有对应文件夹则自动生成\n",
    "    file_name = string_array[0]\n",
    "    with open(\n",
    "        file_address + file_name + \".txt\", \"w\", encoding=\"utf-8\"\n",
    "    ) as f:  # 必须跟解码形式，不然有的网页中文内容写不到txt里\n",
    "        f.write(string_array[1])\n",
    "\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    url = \"https://so.gushiwen.cn/gushi/tangshi.aspx\"  # 古诗文网三国演义网址\n",
    "    url_head_content = get_url_content(url)  # 获取网页\n",
    "    links = get_url_links(url_head_content)  # 获取每一回的链接地址\n",
    "    # enumerate() 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列\n",
    "    # 同时列出数据下标和数据\n",
    "    for index, each in enumerate(links):\n",
    "        url_link_content = get_url_content(each)  # 获取每一回的网页内容\n",
    "        chapter_content = filter_info(index + 1, url_link_content)  # 解析每一回的网页内容，获取小说文本\n",
    "        write_txt(chapter_content)  # 输出小说内容到 txt\n",
    "        time.sleep(random.random() * 1)  # 每抓一个网页休息0~1秒，防止被反爬措施封锁 IP\n",
    "\n",
    "\n",
    "# 运行函数\n",
    "main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9c3386",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "其中，使用 requests 库时，有的网站需要设置 headers 参数才能正确模拟浏览器登录。headers 参数可以在打开网页时，按键盘`F12` 打开， 然后刷新一下网页，单击`F12`弹出窗口的任何一个文件，就能查看网站的所有 headers 信息，有 cookie，user-agent 等信息。如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e89bc7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![sanguo_4.png](_build/html/_images/san_guo4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2d6563",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "我们可以复制一部分信息或全部信息，放到 request 函数的 headers 参数里面。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fa33ca",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 爬虫实例 2：抓豆瓣 top 250 的电影信息\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefbd4a2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "我们想获取豆瓣高分评价的 250 部电影的信息：电影名字，导演，上映年代，评分，评分人数等。打开豆瓣 top 250 的网页链接 https://movie.douban.com/top250?start= ，发现每个网页显示 25 部电影，一共有 10 个挖网页，网页地址与链接中的 start = 后面的数字有关。例如第 3 个子网页的链接地址是 'https://movie.douban.com/top250?start=50 ，根据这个规律，我们可以用一个 for 循环，将 10 个网页分别打开抓取信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ae712e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "打开一个网页，单击右键查看网页源代码，如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cf43e6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![douban1.png](_build/html/_images/douban1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a0a878",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "通过源代码发现以下几个特点：\n",
    "\n",
    "- 每部电影的信息都在标签 &lt;div class='item'&gt; 里面。据此，我们可以用一个 for 循环，将每个网页的  &lt;div class='item'&gt; 标签都检索出来\n",
    "\n",
    "- 电影的中英文名字都在标签  &lt;div class='title'&gt; 里，而其他名字在标签 &lt;div class='other'&gt; 里。我们可以使用函数`find()`以及`find_next()` 检索出来。\n",
    "\n",
    "- 导演，主演，上映年代，电影产地，电影类型信息在标签 &lt;p class=''&gt; 里。我们可以使用函数`find()`检索，然后利用正则表达式将里面对应文本信息分别提取出来。\n",
    "\n",
    "- 评分信息在标签 &lt;div class='rating_num'&gt; 里。我们可以使用函数`find()`检索出来。\n",
    "\n",
    "- 评分人数信息在标签 &lt;span&gt; 里。我们可以通过函数`find()`结合关键字'评价'检索出来。（网页中的 &lt; span &gt; 提供了一种将文本的一部分或者文档的一部分独立出来的方式）\n",
    "\n",
    "- 代表性评价信息在标签 &lt;div class='quote'&gt; 里。我们可以使用函数`find()`检索出来。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472e3e85",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "根据以上的特点，设计出以下的 python 代码。其中中文分词库`jieba`，词云库`WordCloud`库等，若没有安装，需要提前安装（使用 pip 或 conda 安装）到电脑上。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6651abe3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "在画图时，使用 matplotlib 画出饼图，使用 WordCloud 生成词云图，因为 matplotlib 不能自动将不同柱子分配不同的颜色，使用 seaborn 画出柱状图。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45450fad",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```{code-block} python\n",
    "import requests  # 联系网络的包，a package for requesting from websites\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup  # 分析网页数据的包，a package for webstie data analysis\n",
    "import matplotlib.pyplot as plt  # 画图的包\n",
    "import time\n",
    "import random\n",
    "import jieba  # 中文分词包\n",
    "from wordcloud import WordCloud  # 词云包\n",
    "import re  # 正则表达式包\n",
    "import seaborn as sns  # 用 seaborn 画柱状图\n",
    "\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"SimHei\"]\n",
    "\n",
    "# 主要是模拟浏览器登录，防止反爬\n",
    "headers = {\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.108 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# store the information\n",
    "rank_list = []\n",
    "name_list = []  # 用来存储电影名字的一个二维列表\n",
    "director_list = []  # 用来存储导演名字的一个列表，下面几个变量类似\n",
    "actor_list = []\n",
    "year_list = []\n",
    "rating_list = []\n",
    "ratingNum_list = []\n",
    "quote_list = []\n",
    "place_list = []\n",
    "category_list = []\n",
    "\n",
    "num = 0\n",
    "for i in range(0, 10):\n",
    "    link = \"https://movie.douban.com/top250?start=\" + str(\n",
    "        i * 25\n",
    "    )  # 250部电影一共 10个网页， 10 pages for total 250 movies\n",
    "    res = requests.get(link, headers=headers, timeout=10)\n",
    "    time.sleep(\n",
    "        random.random() * 3\n",
    "    )  # 每抓一个网页休息2~3秒，防止被反爬措施封锁 IP，avoid being blocked of the IP address\n",
    "\n",
    "    # res.text is the content of the crawler\n",
    "    soup = BeautifulSoup(\n",
    "        res.text, \"lxml\"\n",
    "    )  # lxml 是一个解码方式，lxml is one decoding model for Beautifulsoup\n",
    "    movie_list = soup.find_all(\"div\", class_=\"item\")  # 检索所有类型为 item 的 div 标签\n",
    "\n",
    "    for each in movie_list:\n",
    "        # 排名在 em 标签里\n",
    "        rank = each.em.text\n",
    "        rank_list.append(rank)\n",
    "\n",
    "        # 电影名字在标签 <span class=\"title\"> 与 <span class=\"other\"> 里\n",
    "        other_name = each.find(\"span\", class_=\"other\").get_text()\n",
    "        other_name = other_name.replace(\" \", \"\")  # 去掉空格\n",
    "        other_name = other_name.replace(\"\\xa0/\\xa0\", \"\")  # 去掉多余字符 \\xa0/\\xa0\n",
    "        cn_name = each.find(\"span\", class_=\"title\").get_text()\n",
    "        eg_name = (\n",
    "            each.find(\"span\", class_=\"title\").find_next().get_text()\n",
    "        )  # find_next() 查找满足条件的下一个标签\n",
    "        eg_name = eg_name.replace(\"\\xa0/\\xa0\", \"\")  # 去掉多余字符 \\xa0/\\xa0\n",
    "        name_list.append([cn_name, eg_name, other_name])\n",
    "\n",
    "        # 评分信息在标签  <span class=\"rating_num\"> 里\n",
    "        rating = each.find(\"span\", class_=\"rating_num\").get_text()\n",
    "        rating_list.append(float(rating))\n",
    "\n",
    "        # 评价人数通过关键词'评价'检索\n",
    "        rating_num = each.find(string=re.compile(\"评价\")).get_text()\n",
    "        rating_num = rating_num.replace(\"人评价\", \"\")\n",
    "        ratingNum_list.append(int(rating_num))\n",
    "\n",
    "        # 代表性评价在  <p class=\"quote\"> 里\n",
    "        try:\n",
    "            quote = each.find(\"p\", class_=\"quote\").get_text()\n",
    "        except Exception:  # 有的电影代表性评价没有显示\n",
    "            quote = \"\"\n",
    "        quote_list.append(quote)\n",
    "\n",
    "        info = each.p.get_text(strip=True)\n",
    "        # 定义正则表达式提取出导演，主演，上映时间，地点，电影类型信息\n",
    "        try:\n",
    "            # (?<=导演: ) match any character begins after character '导演: '\n",
    "\n",
    "            # .*? match any character (.), zero or more times (*) but as less as possible (?)\n",
    "\n",
    "            # (?=主) match any character before character '主'\n",
    "\n",
    "            director = re.compile(\"(?<=导演: ).*?(?=主)\").findall(info)[0]\n",
    "            actor = re.compile(\"(?<=主演: ).*?(?=/)\").findall(info)[0]\n",
    "        except Exception:  # 有的电影导演名字太长，主演没有显示出来\n",
    "            director = re.compile(\"(?<=导演: ).*?(?=\\xa0)\").findall(info)[0]\n",
    "            actor = \"\"\n",
    "        director_list.append(director)\n",
    "        actor_list.append(actor)\n",
    "\n",
    "        # \\d{4} is a four digit number\n",
    "        year = re.compile(\"(?<=...)\\d{4}\").findall(info)[0]\n",
    "        year_list.append(year)\n",
    "        place_category = re.compile(\"(?<=\\xa0/\\xa0).*\").findall(info)[0]\n",
    "        place_category = place_category.replace(\"\\xa0\", \"\")\n",
    "        place_category = place_category.split(\"/\")\n",
    "        place = place_category[0]\n",
    "        category = place_category[1]\n",
    "        place_list.append(place)\n",
    "        category_list.append(category)\n",
    "\n",
    "# 将数据存到 pandas 里\n",
    "df = pd.DataFrame(rank_list, columns=[\"排名\"])\n",
    "df[\"电影名字\"] = [i[0] for i in name_list]\n",
    "df[\"外文名字\"] = [i[1] for i in name_list]\n",
    "df[\"其他名字\"] = [i[2] for i in name_list]\n",
    "df[\"评分\"] = rating_list\n",
    "df[\"评价人数\"] = ratingNum_list\n",
    "df[\"导演\"] = director_list\n",
    "df[\"主演\"] = actor_list\n",
    "df[\"上映日期\"] = year_list\n",
    "df[\"地区\"] = place_list\n",
    "df[\"类型\"] = category_list\n",
    "df[\"代表性评论\"] = quote_list\n",
    "\n",
    "# 导出到 xls 文件里，save to xls file\n",
    "df.to_csv(\"豆瓣 top 250 电影爬虫抓取.csv\")\n",
    "\n",
    "# 分析电影来源地并画饼图\n",
    "locations = []\n",
    "for i in range(len(place_list)):\n",
    "    nations = place_list[i].split(\" \")\n",
    "    for j in range(len(nations)):\n",
    "        if nations[j] == \"西德\":\n",
    "            nations[j] = \"德国\"\n",
    "        locations.append(nations[j])\n",
    "\n",
    "df_location = pd.DataFrame(locations, columns=[\"地区\"])\n",
    "# 按照出现次数排序, size() 可以查数，生成一个 Series 类型\n",
    "# 然后用 reset_index 重新命名，参数为 name\n",
    "# DataFrame 类型的 reset_index 参数为 names\n",
    "\n",
    "df2 = (\n",
    "    df_location.groupby(\"地区\").size().reset_index(name=\"counts\")\n",
    ")  # df2 = df2['地区'].value_counts(ascending = False).reset_index()\n",
    "df2.sort_values(by=\"counts\", ascending=False, inplace=True, ignore_index=True)\n",
    "# 画饼状图\n",
    "\n",
    "values = []\n",
    "labels = []\n",
    "other_count = 0\n",
    "for i in range(9):\n",
    "    values.append(df2[\"counts\"][i])\n",
    "    labels.append(df2[\"地区\"][i])\n",
    "for i in range(9, df2.shape[0]):\n",
    "    other_count += int(df2[\"counts\"][i])\n",
    "values.append(other_count)\n",
    "labels.append(\"其他地区\")\n",
    "plt.figure(1)\n",
    "plt.pie(values, labels=labels, autopct=\"%.2f%%\")\n",
    "plt.legend()  # 显示标签\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 分析电影类型并画饼图\n",
    "categories = []\n",
    "for i in range(len(category_list)):\n",
    "    category = category_list[i].split(\" \")\n",
    "    for j in range(len(category)):\n",
    "        categories.append(category[j])\n",
    "\n",
    "df_category = pd.DataFrame(categories, columns=[\"类型\"])\n",
    "df3 = df_category.groupby(\"类型\").size().reset_index(name=\"counts\")\n",
    "df3.sort_values(by=\"counts\", ascending=False, inplace=True, ignore_index=True)\n",
    "\n",
    "values = []\n",
    "labels = []\n",
    "other_count = 0\n",
    "for i in range(15):\n",
    "    values.append(df3[\"counts\"][i])\n",
    "    labels.append(df3[\"类型\"][i])\n",
    "for i in range(15, df3.shape[0]):\n",
    "    other_count += int(df3[\"counts\"][i])\n",
    "values.append(other_count)\n",
    "labels.append(\"其他类型\")\n",
    "plt.figure(2)\n",
    "plt.pie(values, labels=labels, autopct=\"%.2f%%\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 画词云图\n",
    "jieba.add_word(\"久石让\")\n",
    "jieba.add_word(\"谢耳朵\")\n",
    "\n",
    "# 一些语气词和没有意义的词\n",
    "del_words = [\"就是\", \"一个\",\"被\",\"电影\",\"我们\",\"不是\",\"每个\",\"不会\",\n",
    "    \"没有\",\"这样\",\"那么\",\"不要\",\"如果\",\"不能\",\n",
    "    \"一种\",\"不过\",\"只有\",\"不得不\",\"不得不\",\"一部\",\n",
    "]\n",
    "all_quotes = \"\".join(quote_list)  # 将所有代表性评论拼接为一个文本\n",
    "\n",
    "# 去掉标点符号\n",
    "all_quotes = re.sub(\n",
    "    r\"[0-9\\s+\\.\\!\\/_,$%^*()?;；:-【】+\\\"\\']+|[+——！，;:。？、~@#￥%……&*（）]+\", \" \", all_quotes\n",
    ")\n",
    "words = jieba.lcut(all_quotes)\n",
    "words_final = []\n",
    "for i in range(len(words)):\n",
    "    if len(words[i]) > 1 and words[i] not in del_words:  # 去掉一些语气词，单字词\n",
    "        words_final.append(words[i])\n",
    "text_result = Counter(words_final)\n",
    "cloud = WordCloud(\n",
    "    font_path=\"C:\\Windows\\Fonts\\FZSTK.TTF\",  # 中文字体地址 C:\\Windows\\Fonts\\FZSTK.TTF，提前下载字体或指定，否则中文无法显示\n",
    "    # mask = '' 通过 mask 参数指定一个图片地址作为词云的背景图像\n",
    "    background_color=\"white\",\n",
    "    width=1000,\n",
    "    height=860,\n",
    "    max_words=25,\n",
    ")\n",
    "\n",
    "# wc = cloud.generate(words) # 这种方法对中文支持不太好，this mehtod is better for only english string\n",
    "wc = cloud.generate_from_frequencies(text_result)\n",
    "wc.to_file(\"豆瓣 TOP 250 词云.jpg\")\n",
    "plt.figure(3)\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"豆瓣 TOP 250 电影代表性评论的词云分析\")\n",
    "plt.show()\n",
    "\n",
    "# 评分最高的 15 部电影\n",
    "# 用 seaborn 画柱状图，因为自动将不同柱子分配不同的颜色\n",
    "df_star = df.sort_values(by=\"评分\", ascending=False, ignore_index=True)\n",
    "number = 15\n",
    "df_star_top = df_star.head(number)\n",
    "plt.figure(4)\n",
    "sns.barplot(data=df_star_top, y=\"电影名字\", x=\"评分\", orient=\"h\")\n",
    "plt.title(\"评分最高的 \" + str(number) + \" 部电影\")\n",
    "plt.show()\n",
    "\n",
    "# 评分人数最多的 15 部电影\n",
    "df_num = df.sort_values(by=\"评价人数\", ascending=False, ignore_index=True)\n",
    "df_num_top = df_num.head(number)\n",
    "plt.figure(5)\n",
    "sns.barplot(data=df_num_top, y=\"电影名字\", x=\"评价人数\", orient=\"h\")\n",
    "plt.title(\"评分人数最多的 \" + str(number) + \" 部电影\")\n",
    "plt.show()\n",
    "\n",
    "# 电影年代画图\n",
    "df_year = df.groupby(\"上映日期\").size().reset_index(name=\"部数\")\n",
    "df_year.sort_values(\"部数\", ascending=False, inplace=True)\n",
    "df_year_top = df_year.head(20)\n",
    "df_year_top.sort_values(\"上映日期\", inplace=True)\n",
    "plt.figure(6)\n",
    "sns.barplot(data=df_year_top, x=\"上映日期\", y=\"部数\")\n",
    "plt.title(\"电影上映年代\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941f40d7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "生成的图形如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9699ad1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![douban2.png](_build/html/_images/douban2.png)\n",
    "![douban3.png](_build/html/_images/douban3.png)\n",
    "![douban4.png](_build/html/_images/douban4.png)\n",
    "![douban5.png](_build/html/_images/douban5.png)\n",
    "![douban6.png](_build/html/_images/douban6.png)\n",
    "![douban7.png](_build/html/_images/douban7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f015cdae",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 后记\n",
    "\n",
    "\n",
    "- 不同的网站的爬虫技巧经常不一样，要具体网站具体分析。\n",
    "- 由于一些网站经常更新数据的存储地址或源代码，某段时间的爬虫代码可能在下一个时间段就不能用了，爬虫代码也要更新。\n",
    "- 有的网页信息并不在网站直接打开的网页源代码里（例如弹幕、电商评论等），还要结合按键`F12`找到网页信息的真实地址。\n",
    "- 提取网页信息时，经常要用到正则表达式处理字符串。\n",
    "\n",
    "\n",
    "近年来，许多网站都提高了反爬手段防止网站的数据被无端获取。因此，提醒读者避免过度使用爬虫，给一些网站造成不必要的访问负担。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a15a83d",
   "metadata": {},
   "source": [
    "## 练习\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7cac8e",
   "metadata": {},
   "source": [
    "```{exercise}\n",
    ":label: crab-poem\n",
    "练习从网上抓取一首诗词。\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fbf5b8",
   "metadata": {},
   "source": [
    "```{exercise}\n",
    ":label: crab-news\n",
    "练习抓取学校的新闻，并尝试画出词云图。\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85dded6",
   "metadata": {},
   "source": [
    "```{exercise}\n",
    ":label: crab-红楼\n",
    "尝试用爬虫下载小说《红楼梦》到电脑里。\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8c703b",
   "metadata": {},
   "source": [
    "<script src=\"https://giscus.app/client.js\"\n",
    "        data-repo=\"robinchen121/book--Data-Science\"\n",
    "        data-repo-id=\"R_kgDOKFdyOw\"\n",
    "        data-category=\"Announcements\"\n",
    "        data-category-id=\"DIC_kwDOKFdyO84CgWHi\"\n",
    "        data-mapping=\"pathname\"\n",
    "        data-strict=\"0\"\n",
    "        data-reactions-enabled=\"1\"\n",
    "        data-emit-metadata=\"0\"\n",
    "        data-input-position=\"bottom\"\n",
    "        data-theme=\"light\"\n",
    "        data-lang=\"en\"\n",
    "        crossorigin=\"anonymous\"\n",
    "        async>\n",
    "</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294b7a45",
   "metadata": {},
   "source": [
    "<!-- Toogle google translation -->\n",
    "<div id=\"google_translate_element\"></div>\n",
    "<script type=\"text/javascript\">\n",
    "      function googleTranslateElementInit() {\n",
    "        new google.translate.TranslateElement({ pageLanguage: 'zh-CN',\n",
    "                  includedLanguages: 'en,zh-CN,zh-TW,ja,ko,de,ru,fr,es,it,pt,hi,ar,fa',\n",
    "layout: google.translate.TranslateElement.InlineLayout.SIMPLE }, 'google_translate_element');\n",
    "      }\n",
    "</script>\n",
    "<script type=\"text/javascript\"\n",
    "      src=\"https://translate.google.com/translate_a/element.js?cb=googleTranslateElementInit\"\n",
    "></script>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "幻灯片",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}